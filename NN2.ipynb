{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4bm/Xrx6s/3KNLx1rF8Ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marta4cod/NNProject2/blob/main/NN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlTxbwrQdXQU",
        "outputId": "523a451b-ddf8-44e6-d490-c242625474d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k0vJSn7Tdy7g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6ABIotpMd7PI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fKjjlRUeIzV",
        "outputId": "cddd7be6-7da6-4824-8b9c-049b1d3f9a38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'NNProject2' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pDtNAi1oeOYr",
        "outputId": "ad84b82b-5c02-48a3-88b3-5b49806772ab"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NNProject2\n",
            "[main 762ded1] Add notebook from Colab\n",
            " Date: Thu Apr 3 12:01:04 2025 +0000\n",
            " 1 file changed, 1 insertion(+)\n",
            " create mode 100644 NN2.ipynb\n",
            "Enumerating objects: 3, done.\n",
            "Counting objects: 100% (3/3), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (2/2), done.\n",
            "Writing objects: 100% (3/3), 1.59 KiB | 1.59 MiB/s, done.\n",
            "Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/marta4cod/NNProject2.git\n",
            " * [new branch]      main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "uvGz4Pb5hisA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import numpy as np\n",
        "import string, re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering\n",
        ")\n"
      ],
      "metadata": {
        "id": "fZsP_cSHiH50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# CONFIG\n",
        "###############################################################################\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2  # Increase for better results\n",
        "LEARNING_RATE = 3e-5\n",
        "\n",
        "# Distillation hyperparams\n",
        "ALPHA = 0.5        # Weight of soft (teacher) loss\n",
        "TEMPERATURE = 3.0  # Temperature for soft logits\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "metadata": {
        "id": "D_G5b1khjpYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# 1) DATA LOADING & PREPROCESSING\n",
        "###############################################################################\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "\n",
        "# For demonstration, we'll use the full training set or a subset\n",
        "train_data = raw_squad[\"train\"]\n",
        "val_data = raw_squad[\"validation\"].select(range(1000))  # Subset of 1k for speed\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess_function(ex):\n",
        "    \"\"\"\n",
        "    Tokenize question + context and try to map answer start/end to token indices.\n",
        "    We'll do a naive single-chunk approach (no sliding window).\n",
        "    \"\"\"\n",
        "    # SQuAD \"answers\" has a list of possible answers; we take the first\n",
        "    start_char = ex[\"answers\"][\"answer_start\"][0]\n",
        "    ans_texts = ex[\"answers\"][\"text\"]\n",
        "    answer_text = ans_texts[0] if len(ans_texts) > 0 else \"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        ex[\"question\"],\n",
        "        ex[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True  # We'll use offsets for naive char->token mapping\n",
        "    )\n",
        "\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "\n",
        "    # Find start/end token indices\n",
        "    start_token_idx = 0\n",
        "    end_token_idx = 0\n",
        "\n",
        "    # End char\n",
        "    end_char = start_char + len(answer_text)\n",
        "\n",
        "    # loop through offsets to find the best match\n",
        "    for i, (off_start, off_end) in enumerate(offsets):\n",
        "        # Some offsets may be None or special tokens\n",
        "        if off_start is None or off_end is None:\n",
        "            continue\n",
        "        if off_start <= start_char < off_end:\n",
        "            start_token_idx = i\n",
        "        if off_start < end_char <= off_end:\n",
        "            end_token_idx = i\n",
        "            break\n",
        "\n",
        "    if end_token_idx < start_token_idx:\n",
        "        end_token_idx = start_token_idx\n",
        "\n",
        "    # Store in encoding\n",
        "    encoding[\"start_positions\"] = start_token_idx\n",
        "    encoding[\"end_positions\"] = end_token_idx\n",
        "\n",
        "    # Remove offset mapping to reduce data size\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    return encoding\n",
        "\n",
        "train_processed = train_data.map(preprocess_function)\n",
        "val_processed   = val_data.map(preprocess_function)\n",
        "\n",
        "# We'll convert to PyTorch Tensors\n",
        "def to_tensor_dataset(hf_dataset):\n",
        "    input_ids = torch.tensor(hf_dataset[\"input_ids\"], dtype=torch.long)\n",
        "    attention_mask = torch.tensor(hf_dataset[\"attention_mask\"], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor(hf_dataset[\"token_type_ids\"], dtype=torch.long) \\\n",
        "        if \"token_type_ids\" in hf_dataset.features else None\n",
        "    start_positions = torch.tensor(hf_dataset[\"start_positions\"], dtype=torch.long)\n",
        "    end_positions   = torch.tensor(hf_dataset[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        return TensorDataset(input_ids, attention_mask, token_type_ids, start_positions, end_positions)\n",
        "    else:\n",
        "        # For models without token_type_ids (like DistilBERT)\n",
        "        return TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "\n",
        "train_tds = to_tensor_dataset(train_processed)\n",
        "val_tds   = to_tensor_dataset(val_processed)\n",
        "\n",
        "train_loader = DataLoader(train_tds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader   = DataLoader(val_tds,   batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EI2tXAXJjtZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 2) TEACHER MODEL INFERENCE (collect teacher logits)\n",
        "###############################################################################\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "teacher_start_logits_list = []\n",
        "teacher_end_logits_list   = []\n",
        "gt_start_list = []\n",
        "gt_end_list   = []\n",
        "input_tensors = []\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "      count+=1\n",
        "      print(\"batch\", count)\n",
        "      # batch can have 4 or 5 tensors depending on token_type_ids existence\n",
        "      if len(batch) == 5:\n",
        "          input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "          token_type_ids = token_type_ids.to(DEVICE)\n",
        "      else:\n",
        "          input_ids, attention_mask, start_pos, end_pos = batch\n",
        "          token_type_ids = None\n",
        "\n",
        "      input_ids = input_ids.to(DEVICE)\n",
        "      attention_mask = attention_mask.to(DEVICE)\n",
        "      start_pos = start_pos.to(DEVICE)\n",
        "      end_pos   = end_pos.to(DEVICE)\n",
        "\n",
        "      outputs = teacher_model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids\n",
        "      )\n",
        "      # Collect logits\n",
        "      teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "      teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "\n",
        "      gt_start_list.append(start_pos.cpu())\n",
        "      gt_end_list.append(end_pos.cpu())\n",
        "\n",
        "      # We'll store the CPU tensors of inputs for the student dataset\n",
        "      if token_type_ids is not None:\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu()))\n",
        "      else:\n",
        "          # Rare for BERT-based QA to not have token_type_ids, but just in case:\n",
        "          # We'll store None in place of token_type_ids.\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), None))\n",
        "\n",
        "# Concatenate teacher outputs\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full   = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full   = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten input tensors\n",
        "all_input_ids         = []\n",
        "all_attention_masks   = []\n",
        "all_token_type_ids    = []\n",
        "for batch_data in input_tensors:\n",
        "    i_ids, i_mask, i_type = batch_data\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "    else:\n",
        "        # If there's no token_type_ids, we store None\n",
        "        all_token_type_ids = None\n",
        "\n",
        "all_input_ids       = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "\n",
        "print(\"Teacher logits shapes:\")\n",
        "print(\"  start_logits:\", teacher_start_logits_full.shape)\n",
        "print(\"  end_logits:  \", teacher_end_logits_full.shape)\n",
        "print(\"Ground truth shapes:\", gt_start_full.shape, gt_end_full.shape)"
      ],
      "metadata": {
        "id": "aET8hRx1j0Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# 3) DEFINE A SMALLER STUDENT MODEL\n",
        "###############################################################################\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,         # smaller hidden dim\n",
        "    num_hidden_layers=8,     # fewer layers\n",
        "    num_attention_heads=8,   # fewer heads\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)"
      ],
      "metadata": {
        "id": "I19QFe_6j7fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 4) DISTILLATION TRAINING\n",
        "###############################################################################\n",
        "# We'll build a custom dataset that yields inputs & teacher (soft) + ground-truth (hard)\n",
        "# We'll store teacher's probabilities (soft targets) with a temperature-based softmax.\n",
        "\n",
        "def softmax_with_temperature(logits, temperature=TEMPERATURE):\n",
        "    # logits: [batch_size, seq_len]\n",
        "    return F.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs   = softmax_with_temperature(teacher_end_logits_full,   TEMPERATURE)\n",
        "\n",
        "class DistillationDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids,\n",
        "                 teacher_start_probs, teacher_end_probs,\n",
        "                 gt_start, gt_end):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.teacher_start_probs = teacher_start_probs\n",
        "        self.teacher_end_probs   = teacher_end_probs\n",
        "        self.gt_start = gt_start\n",
        "        self.gt_end   = gt_end\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.token_type_ids is not None:\n",
        "            return (\n",
        "                self.input_ids[idx],\n",
        "                self.attention_mask[idx],\n",
        "                self.token_type_ids[idx],\n",
        "                self.teacher_start_probs[idx],\n",
        "                self.teacher_end_probs[idx],\n",
        "                self.gt_start[idx],\n",
        "                self.gt_end[idx],\n",
        "            )\n",
        "        else:\n",
        "            # If no token_type_ids exist\n",
        "            return (\n",
        "                self.input_ids[idx],\n",
        "                self.attention_mask[idx],\n",
        "                None,\n",
        "                self.teacher_start_probs[idx],\n",
        "                self.teacher_end_probs[idx],\n",
        "                self.gt_start[idx],\n",
        "                self.gt_end[idx],\n",
        "            )\n",
        "\n",
        "distill_dataset = DistillationDataset(\n",
        "    all_input_ids, all_attention_masks, all_token_type_ids,\n",
        "    teacher_start_probs, teacher_end_probs,\n",
        "    gt_start_full, gt_end_full\n",
        ")\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def distillation_train_step(batch_data):\n",
        "    if all_token_type_ids is not None:\n",
        "        input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "    else:\n",
        "        input_ids, attention_mask, _, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = None\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "    t_start_probs = t_start_probs.to(DEVICE)\n",
        "    t_end_probs   = t_end_probs.to(DEVICE)\n",
        "    gt_start = gt_start.to(DEVICE)\n",
        "    gt_end   = gt_end.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = student_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "    student_start_logits = outputs.start_logits\n",
        "    student_end_logits   = outputs.end_logits\n",
        "\n",
        "    # 1) Hard loss (CE with ground truth)\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_start_hard = ce_loss_fn(student_start_logits, gt_start)\n",
        "    loss_end_hard   = ce_loss_fn(student_end_logits,   gt_end)\n",
        "    hard_loss = 0.5 * (loss_start_hard + loss_end_hard)\n",
        "\n",
        "    # 2) Soft loss (KL divergence with teacher distribution)\n",
        "    # Convert student logits to probabilities with same temperature\n",
        "    s_start_probs = softmax_with_temperature(student_start_logits, TEMPERATURE)\n",
        "    s_end_probs   = softmax_with_temperature(student_end_logits,   TEMPERATURE)\n",
        "\n",
        "    # We'll use KLDivLoss (expects log probs for input by default)\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    start_kl = kl_loss_fn(s_start_probs.log(), t_start_probs)  # KL(student||teacher)\n",
        "    end_kl   = kl_loss_fn(s_end_probs.log(),   t_end_probs)\n",
        "    soft_loss = 0.5 * (start_kl + end_kl)\n",
        "\n",
        "    loss = ALPHA * soft_loss + (1 - ALPHA) * hard_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), hard_loss.item(), soft_loss.item()\n",
        "\n",
        "# Run distillation training\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n=== EPOCH {epoch+1}/{EPOCHS} ===\")\n",
        "    epoch_losses = []\n",
        "    for step, batch_data in enumerate(distill_loader):\n",
        "        loss_val, hard_val, soft_val = distillation_train_step(batch_data)\n",
        "        epoch_losses.append(loss_val)\n",
        "        if step % 200 == 0:\n",
        "            print(f\" Step {step} - Distill Loss: {loss_val:.4f} (Hard: {hard_val:.4f}, Soft: {soft_val:.4f})\")\n",
        "    print(f\" Average Loss: {np.mean(epoch_losses):.4f}\")"
      ],
      "metadata": {
        "id": "i4Gg4Z1xkBhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5) POST-PROCESSING & EVALUATION\n",
        "###############################################################################\n",
        "# We'll do a naive approach: take argmax of start/end, decode, compare with ground truth.\n",
        "val_contexts = val_data[\"context\"]\n",
        "val_questions = val_data[\"question\"]\n",
        "val_answers = val_data[\"answers\"]  # list of dicts with \"text\", \"answer_start\"\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return \"\".join(ch for ch in txt if ch not in string.punctuation)\n",
        "\n",
        "    s = s.lower()\n",
        "    s = remove_articles(s)\n",
        "    s = remove_punc(s)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "student_model.eval()\n",
        "em_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "val_dataloader = DataLoader(val_tds, batch_size=8, shuffle=False)\n",
        "offset = 0  # to track the global index in val_data\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        outputs = student_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits   = outputs.end_logits\n",
        "\n",
        "        start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "        end_indices   = torch.argmax(end_logits,   dim=1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(start_indices)):\n",
        "            global_idx = offset + i\n",
        "            if global_idx >= len(val_contexts):\n",
        "                continue\n",
        "\n",
        "            s_ind = start_indices[i]\n",
        "            e_ind = end_indices[i]\n",
        "            if e_ind < s_ind:\n",
        "                e_ind = s_ind\n",
        "\n",
        "            # Decode predicted tokens\n",
        "            tokens_ = input_ids[i][s_ind : e_ind+1].cpu().numpy().tolist()\n",
        "            pred_text = tokenizer.decode(tokens_, skip_special_tokens=True)\n",
        "\n",
        "            # Ground truth: we pick the first answer\n",
        "            gold_answers = val_answers[global_idx][\"text\"]\n",
        "            if len(gold_answers) > 0:\n",
        "                gold_answer = gold_answers[0]\n",
        "            else:\n",
        "                gold_answer = \"\"\n",
        "\n",
        "            em = compute_exact_match(pred_text, gold_answer)\n",
        "            f1 = compute_f1(pred_text, gold_answer)\n",
        "\n",
        "            em_scores.append(em)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "        offset += len(start_indices)\n",
        "\n",
        "avg_em = np.mean(em_scores) * 100\n",
        "avg_f1 = np.mean(f1_scores) * 100\n",
        "print(f\"\\nValidation Results (subset of 1000 samples):\")\n",
        "print(f\"  Exact Match: {avg_em:.2f}%\")\n",
        "print(f\"  F1 Score:    {avg_f1:.2f}%\")"
      ],
      "metadata": {
        "id": "aNePOmBgkObU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def ask_question(question: str, context: str, model, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Given a question and a context, use the provided model to\n",
        "    predict the answer span and return the decoded string answer.\n",
        "    \"\"\"\n",
        "    # 1) Encode inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    token_type_ids = None\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # 2) Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # 3) Get predicted start/end token indices\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Ensure the end_index is >= start_index\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    # 4) Decode tokens back to string\n",
        "    answer_ids = input_ids[0, start_index : end_index+1]\n",
        "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    return answer_text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "\n",
        "# Suppose you have:\n",
        "#   teacher_model, student_model (both on the same device, e.g., \"cuda\" or \"cpu\")\n",
        "#   tokenizer (matching your BERT-based QA model)\n",
        "# Example question + context:\n",
        "#question = \"What is the capital of France?\"\n",
        "#context = \"France is a country in Europe. Its largest city and capital is Paris. It is known for the Eiffel Tower.\"\n",
        "#question = \"Which country is Middlesex University based?\"\n",
        "#question = \"Is Middlesex University a public or an independent university?\"\n",
        "#context = \"Middlesex University London is a public research university based in Hendon, northwest London, England.\"\n",
        "question = \"Which city is Galatasaray based in?\"\n",
        "context = \"Galatasaray, is a Turkish professional football club based on the European side of the city of Istanbul. It is founded in 1905. The team traditionally play in dark shades of red and yellow at home.\"\n",
        "\n",
        "# Evaluate with teacher model\n",
        "teacher_model.eval()\n",
        "teacher_answer = ask_question(question, context, teacher_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Teacher Answer]: {teacher_answer}\")\n",
        "\n",
        "# Evaluate with student model\n",
        "student_model.eval()\n",
        "student_answer = ask_question(question, context, student_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Student Answer]: {student_answer}\")"
      ],
      "metadata": {
        "id": "IBd4vVy1kPmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPTxA2b3kVz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}