{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXQB48nSfu/bCOlaqogMUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61bba3edb07c45149a16e24c6e55e465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43377bb1005641d9a8a41e45b085861c",
              "IPY_MODEL_0b555d3073ac4765ba15c197e9ba61b8",
              "IPY_MODEL_be4219bb47cf4261b188aebc0a4d1e71"
            ],
            "layout": "IPY_MODEL_db4d97d31d5347389ce735c0ab9d438c"
          }
        },
        "43377bb1005641d9a8a41e45b085861c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f4361fdde8d4ef1bb1a8c8febfd07c3",
            "placeholder": "​",
            "style": "IPY_MODEL_bafa2963c7c94950909ba04c408c884f",
            "value": "Map: 100%"
          }
        },
        "0b555d3073ac4765ba15c197e9ba61b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5b9bcd8a4de4307a66d0ddcc719c4c9",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9e1f90a7844050bbcce436b8dd1676",
            "value": 1000
          }
        },
        "be4219bb47cf4261b188aebc0a4d1e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2adb5f1750d64cacbbce1ba7a5c2c539",
            "placeholder": "​",
            "style": "IPY_MODEL_03a2bf4ccdac4500bc7c98cb7679c43f",
            "value": " 1000/1000 [00:01&lt;00:00, 732.74 examples/s]"
          }
        },
        "db4d97d31d5347389ce735c0ab9d438c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4361fdde8d4ef1bb1a8c8febfd07c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bafa2963c7c94950909ba04c408c884f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5b9bcd8a4de4307a66d0ddcc719c4c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9e1f90a7844050bbcce436b8dd1676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2adb5f1750d64cacbbce1ba7a5c2c539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a2bf4ccdac4500bc7c98cb7679c43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94814266c7b647e39ce744ee22a11f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_398d5f9fbd4e4518bda7995dcde97825",
              "IPY_MODEL_cc35665756164a53aa286f7b412cf59b",
              "IPY_MODEL_d5c6e364c6a74ccbb8481e6338a79f4e"
            ],
            "layout": "IPY_MODEL_b1148efaa7ee4cbbae2f10b29c21e4fd"
          }
        },
        "398d5f9fbd4e4518bda7995dcde97825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0453b24d539446c9a3ce751be9b62779",
            "placeholder": "​",
            "style": "IPY_MODEL_efe3e81f03ba4532b3abbfb82745c0f8",
            "value": "Map: 100%"
          }
        },
        "cc35665756164a53aa286f7b412cf59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee465625c4443ed94f8b68b7ffa6ca7",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eed3594d1011407c9dfbd60a26ec98ca",
            "value": 200
          }
        },
        "d5c6e364c6a74ccbb8481e6338a79f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af264b660f7a4255946b978fbfa5933a",
            "placeholder": "​",
            "style": "IPY_MODEL_936ef550c66047fa8eb3163a51a8e735",
            "value": " 200/200 [00:00&lt;00:00, 855.67 examples/s]"
          }
        },
        "b1148efaa7ee4cbbae2f10b29c21e4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0453b24d539446c9a3ce751be9b62779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe3e81f03ba4532b3abbfb82745c0f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ee465625c4443ed94f8b68b7ffa6ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed3594d1011407c9dfbd60a26ec98ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af264b660f7a4255946b978fbfa5933a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936ef550c66047fa8eb3163a51a8e735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marta4cod/NNProject2/blob/main/NN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uvGz4Pb5hisA",
        "outputId": "3c94e625-8a51-437a-9fd6-5ab5e214c29c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import numpy as np\n",
        "import string, re\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering\n",
        ")\n"
      ],
      "metadata": {
        "id": "fZsP_cSHiH50"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# CONFIG\n",
        "###############################################################################\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2  # Increase for better results\n",
        "LEARNING_RATE = 3e-5\n",
        "\n",
        "# Distillation hyperparams\n",
        "ALPHA = 0.5        # Weight of soft (teacher) loss\n",
        "TEMPERATURE = 3.0  # Temperature for soft logits\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_G5b1khjpYB",
        "outputId": "48d9be04-50b2-45d9-b0f0-7cc3a68ff2c9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# 1) DATA LOADING & PREPROCESSING\n",
        "###############################################################################\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "\n",
        "# For demonstration, we'll use the full training set or a subset\n",
        "train_data = raw_squad[\"train\"].select(range(1000))       # 1k examples\n",
        "val_data   = raw_squad[\"validation\"].select(range(200))   # 200 examples\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess_function(ex):\n",
        "    \"\"\"\n",
        "    Tokenize question + context and try to map answer start/end to token indices.\n",
        "    We'll do a naive single-chunk approach (no sliding window).\n",
        "    \"\"\"\n",
        "    # SQuAD \"answers\" has a list of possible answers; we take the first\n",
        "    start_char = ex[\"answers\"][\"answer_start\"][0]\n",
        "    ans_texts = ex[\"answers\"][\"text\"]\n",
        "    answer_text = ans_texts[0] if len(ans_texts) > 0 else \"\"\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        ex[\"question\"],\n",
        "        ex[\"context\"],\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_offsets_mapping=True  # We'll use offsets for naive char->token mapping\n",
        "    )\n",
        "\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "\n",
        "    # Find start/end token indices\n",
        "    start_token_idx = 0\n",
        "    end_token_idx = 0\n",
        "\n",
        "    # End char\n",
        "    end_char = start_char + len(answer_text)\n",
        "\n",
        "    # loop through offsets to find the best match\n",
        "    for i, (off_start, off_end) in enumerate(offsets):\n",
        "        # Some offsets may be None or special tokens\n",
        "        if off_start is None or off_end is None:\n",
        "            continue\n",
        "        if off_start <= start_char < off_end:\n",
        "            start_token_idx = i\n",
        "        if off_start < end_char <= off_end:\n",
        "            end_token_idx = i\n",
        "            break\n",
        "\n",
        "    if end_token_idx < start_token_idx:\n",
        "        end_token_idx = start_token_idx\n",
        "\n",
        "    # Store in encoding\n",
        "    encoding[\"start_positions\"] = start_token_idx\n",
        "    encoding[\"end_positions\"] = end_token_idx\n",
        "\n",
        "    # Remove offset mapping to reduce data size\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    return encoding\n",
        "\n",
        "train_processed = train_data.map(preprocess_function)\n",
        "val_processed   = val_data.map(preprocess_function)\n",
        "\n",
        "# We'll convert to PyTorch Tensors\n",
        "def to_tensor_dataset(hf_dataset):\n",
        "    input_ids = torch.tensor(hf_dataset[\"input_ids\"], dtype=torch.long)\n",
        "    attention_mask = torch.tensor(hf_dataset[\"attention_mask\"], dtype=torch.long)\n",
        "    token_type_ids = torch.tensor(hf_dataset[\"token_type_ids\"], dtype=torch.long) \\\n",
        "        if \"token_type_ids\" in hf_dataset.features else None\n",
        "    start_positions = torch.tensor(hf_dataset[\"start_positions\"], dtype=torch.long)\n",
        "    end_positions   = torch.tensor(hf_dataset[\"end_positions\"], dtype=torch.long)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        return TensorDataset(input_ids, attention_mask, token_type_ids, start_positions, end_positions)\n",
        "    else:\n",
        "        # For models without token_type_ids (like DistilBERT)\n",
        "        return TensorDataset(input_ids, attention_mask, start_positions, end_positions)\n",
        "\n",
        "train_tds = to_tensor_dataset(train_processed)\n",
        "val_tds   = to_tensor_dataset(val_processed)\n",
        "\n",
        "train_loader = DataLoader(train_tds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_loader   = DataLoader(val_tds,   batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "61bba3edb07c45149a16e24c6e55e465",
            "43377bb1005641d9a8a41e45b085861c",
            "0b555d3073ac4765ba15c197e9ba61b8",
            "be4219bb47cf4261b188aebc0a4d1e71",
            "db4d97d31d5347389ce735c0ab9d438c",
            "1f4361fdde8d4ef1bb1a8c8febfd07c3",
            "bafa2963c7c94950909ba04c408c884f",
            "f5b9bcd8a4de4307a66d0ddcc719c4c9",
            "de9e1f90a7844050bbcce436b8dd1676",
            "2adb5f1750d64cacbbce1ba7a5c2c539",
            "03a2bf4ccdac4500bc7c98cb7679c43f",
            "94814266c7b647e39ce744ee22a11f0c",
            "398d5f9fbd4e4518bda7995dcde97825",
            "cc35665756164a53aa286f7b412cf59b",
            "d5c6e364c6a74ccbb8481e6338a79f4e",
            "b1148efaa7ee4cbbae2f10b29c21e4fd",
            "0453b24d539446c9a3ce751be9b62779",
            "efe3e81f03ba4532b3abbfb82745c0f8",
            "8ee465625c4443ed94f8b68b7ffa6ca7",
            "eed3594d1011407c9dfbd60a26ec98ca",
            "af264b660f7a4255946b978fbfa5933a",
            "936ef550c66047fa8eb3163a51a8e735"
          ]
        },
        "collapsed": true,
        "id": "EI2tXAXJjtZP",
        "outputId": "f2dfee2d-56e3-4c3b-bf65-e04e483a891a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61bba3edb07c45149a16e24c6e55e465"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94814266c7b647e39ce744ee22a11f0c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 2) TEACHER MODEL INFERENCE (collect teacher logits)\n",
        "###############################################################################\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "teacher_start_logits_list = []\n",
        "teacher_end_logits_list   = []\n",
        "gt_start_list = []\n",
        "gt_end_list   = []\n",
        "input_tensors = []\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "      count+=1\n",
        "      print(\"batch\", count)\n",
        "      # batch can have 4 or 5 tensors depending on token_type_ids existence\n",
        "      if len(batch) == 5:\n",
        "          input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "          token_type_ids = token_type_ids.to(DEVICE)\n",
        "      else:\n",
        "          input_ids, attention_mask, start_pos, end_pos = batch\n",
        "          token_type_ids = None\n",
        "\n",
        "      input_ids = input_ids.to(DEVICE)\n",
        "      attention_mask = attention_mask.to(DEVICE)\n",
        "      start_pos = start_pos.to(DEVICE)\n",
        "      end_pos   = end_pos.to(DEVICE)\n",
        "\n",
        "      outputs = teacher_model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids\n",
        "      )\n",
        "      # Collect logits\n",
        "      teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "      teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "\n",
        "      gt_start_list.append(start_pos.cpu())\n",
        "      gt_end_list.append(end_pos.cpu())\n",
        "\n",
        "      # We'll store the CPU tensors of inputs for the student dataset\n",
        "      if token_type_ids is not None:\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu()))\n",
        "      else:\n",
        "          # Rare for BERT-based QA to not have token_type_ids, but just in case:\n",
        "          # We'll store None in place of token_type_ids.\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), None))\n",
        "\n",
        "# Concatenate teacher outputs\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full   = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full   = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten input tensors\n",
        "all_input_ids         = []\n",
        "all_attention_masks   = []\n",
        "all_token_type_ids    = []\n",
        "for batch_data in input_tensors:\n",
        "    i_ids, i_mask, i_type = batch_data\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "    else:\n",
        "        # If there's no token_type_ids, we store None\n",
        "        all_token_type_ids = None\n",
        "\n",
        "all_input_ids       = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "\n",
        "print(\"Teacher logits shapes:\")\n",
        "print(\"  start_logits:\", teacher_start_logits_full.shape)\n",
        "print(\"  end_logits:  \", teacher_end_logits_full.shape)\n",
        "print(\"Ground truth shapes:\", gt_start_full.shape, gt_end_full.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aET8hRx1j0Zt",
        "outputId": "6307f73f-d5f6-4bf3-be97-9e1a52ec7d72"
      },
      "execution_count": 29,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 1\n",
            "batch 2\n",
            "batch 3\n",
            "batch 4\n",
            "batch 5\n",
            "batch 6\n",
            "batch 7\n",
            "batch 8\n",
            "batch 9\n",
            "batch 10\n",
            "batch 11\n",
            "batch 12\n",
            "batch 13\n",
            "batch 14\n",
            "batch 15\n",
            "batch 16\n",
            "batch 17\n",
            "batch 18\n",
            "batch 19\n",
            "batch 20\n",
            "batch 21\n",
            "batch 22\n",
            "batch 23\n",
            "batch 24\n",
            "batch 25\n",
            "batch 26\n",
            "batch 27\n",
            "batch 28\n",
            "batch 29\n",
            "batch 30\n",
            "batch 31\n",
            "batch 32\n",
            "batch 33\n",
            "batch 34\n",
            "batch 35\n",
            "batch 36\n",
            "batch 37\n",
            "batch 38\n",
            "batch 39\n",
            "batch 40\n",
            "batch 41\n",
            "batch 42\n",
            "batch 43\n",
            "batch 44\n",
            "batch 45\n",
            "batch 46\n",
            "batch 47\n",
            "batch 48\n",
            "batch 49\n",
            "batch 50\n",
            "batch 51\n",
            "batch 52\n",
            "batch 53\n",
            "batch 54\n",
            "batch 55\n",
            "batch 56\n",
            "batch 57\n",
            "batch 58\n",
            "batch 59\n",
            "batch 60\n",
            "batch 61\n",
            "batch 62\n",
            "batch 63\n",
            "batch 64\n",
            "batch 65\n",
            "batch 66\n",
            "batch 67\n",
            "batch 68\n",
            "batch 69\n",
            "batch 70\n",
            "batch 71\n",
            "batch 72\n",
            "batch 73\n",
            "batch 74\n",
            "batch 75\n",
            "batch 76\n",
            "batch 77\n",
            "batch 78\n",
            "batch 79\n",
            "batch 80\n",
            "batch 81\n",
            "batch 82\n",
            "batch 83\n",
            "batch 84\n",
            "batch 85\n",
            "batch 86\n",
            "batch 87\n",
            "batch 88\n",
            "batch 89\n",
            "batch 90\n",
            "batch 91\n",
            "batch 92\n",
            "batch 93\n",
            "batch 94\n",
            "batch 95\n",
            "batch 96\n",
            "batch 97\n",
            "batch 98\n",
            "batch 99\n",
            "batch 100\n",
            "batch 101\n",
            "batch 102\n",
            "batch 103\n",
            "batch 104\n",
            "batch 105\n",
            "batch 106\n",
            "batch 107\n",
            "batch 108\n",
            "batch 109\n",
            "batch 110\n",
            "batch 111\n",
            "batch 112\n",
            "batch 113\n",
            "batch 114\n",
            "batch 115\n",
            "batch 116\n",
            "batch 117\n",
            "batch 118\n",
            "batch 119\n",
            "batch 120\n",
            "batch 121\n",
            "batch 122\n",
            "batch 123\n",
            "batch 124\n",
            "batch 125\n",
            "Teacher logits shapes:\n",
            "  start_logits: torch.Size([1000, 384])\n",
            "  end_logits:   torch.Size([1000, 384])\n",
            "Ground truth shapes: torch.Size([1000]) torch.Size([1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SAVE TEACHER LOGITS & INPUTS ===\n",
        "import torch\n",
        "\n",
        "save_path = \"teacher_logits_data.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"start_logits\": teacher_start_logits_full,\n",
        "    \"end_logits\": teacher_end_logits_full,\n",
        "    \"gt_start\": gt_start_full,\n",
        "    \"gt_end\": gt_end_full,\n",
        "    \"input_ids\": all_input_ids,\n",
        "    \"attention_mask\": all_attention_masks,\n",
        "    \"token_type_ids\": all_token_type_ids\n",
        "}, save_path)\n",
        "\n",
        "print(f\"✅ Saved teacher logits and data to: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opnCFsPr9n4O",
        "outputId": "6f07a11c-7cf8-401c-bdee-a85970553904"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved teacher logits and data to: teacher_logits_data.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOAD SAVED LOGITS & INPUTS ===\n",
        "import torch\n",
        "\n",
        "load_path = \"teacher_logits_data.pt\"\n",
        "data = torch.load(load_path)\n",
        "\n",
        "teacher_start_logits_full = data[\"start_logits\"]\n",
        "teacher_end_logits_full   = data[\"end_logits\"]\n",
        "gt_start_full             = data[\"gt_start\"]\n",
        "gt_end_full               = data[\"gt_end\"]\n",
        "all_input_ids             = data[\"input_ids\"]\n",
        "all_attention_masks       = data[\"attention_mask\"]\n",
        "all_token_type_ids        = data[\"token_type_ids\"]\n",
        "\n",
        "print(f\"✅ Loaded teacher logits and input data from: {load_path}\")"
      ],
      "metadata": {
        "id": "eGIBOKCg91sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###############################################################################\n",
        "# 3) DEFINE A SMALLER STUDENT MODEL\n",
        "###############################################################################\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,         # smaller hidden dim\n",
        "    num_hidden_layers=8,     # fewer layers\n",
        "    num_attention_heads=8,   # fewer heads\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)"
      ],
      "metadata": {
        "id": "I19QFe_6j7fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# 4) DISTILLATION TRAINING\n",
        "###############################################################################\n",
        "# We'll build a custom dataset that yields inputs & teacher (soft) + ground-truth (hard)\n",
        "# We'll store teacher's probabilities (soft targets) with a temperature-based softmax.\n",
        "\n",
        "def softmax_with_temperature(logits, temperature=TEMPERATURE):\n",
        "    # logits: [batch_size, seq_len]\n",
        "    return F.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs   = softmax_with_temperature(teacher_end_logits_full,   TEMPERATURE)\n",
        "\n",
        "class DistillationDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids,\n",
        "                 teacher_start_probs, teacher_end_probs,\n",
        "                 gt_start, gt_end):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.teacher_start_probs = teacher_start_probs\n",
        "        self.teacher_end_probs   = teacher_end_probs\n",
        "        self.gt_start = gt_start\n",
        "        self.gt_end   = gt_end\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.token_type_ids is not None:\n",
        "            return (\n",
        "                self.input_ids[idx],\n",
        "                self.attention_mask[idx],\n",
        "                self.token_type_ids[idx],\n",
        "                self.teacher_start_probs[idx],\n",
        "                self.teacher_end_probs[idx],\n",
        "                self.gt_start[idx],\n",
        "                self.gt_end[idx],\n",
        "            )\n",
        "        else:\n",
        "            # If no token_type_ids exist\n",
        "            return (\n",
        "                self.input_ids[idx],\n",
        "                self.attention_mask[idx],\n",
        "                None,\n",
        "                self.teacher_start_probs[idx],\n",
        "                self.teacher_end_probs[idx],\n",
        "                self.gt_start[idx],\n",
        "                self.gt_end[idx],\n",
        "            )\n",
        "\n",
        "distill_dataset = DistillationDataset(\n",
        "    all_input_ids, all_attention_masks, all_token_type_ids,\n",
        "    teacher_start_probs, teacher_end_probs,\n",
        "    gt_start_full, gt_end_full\n",
        ")\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def distillation_train_step(batch_data):\n",
        "    if all_token_type_ids is not None:\n",
        "        input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "    else:\n",
        "        input_ids, attention_mask, _, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "        token_type_ids = None\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "    t_start_probs = t_start_probs.to(DEVICE)\n",
        "    t_end_probs   = t_end_probs.to(DEVICE)\n",
        "    gt_start = gt_start.to(DEVICE)\n",
        "    gt_end   = gt_end.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = student_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "    student_start_logits = outputs.start_logits\n",
        "    student_end_logits   = outputs.end_logits\n",
        "\n",
        "    # 1) Hard loss (CE with ground truth)\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_start_hard = ce_loss_fn(student_start_logits, gt_start)\n",
        "    loss_end_hard   = ce_loss_fn(student_end_logits,   gt_end)\n",
        "    hard_loss = 0.5 * (loss_start_hard + loss_end_hard)\n",
        "\n",
        "    # 2) Soft loss (KL divergence with teacher distribution)\n",
        "    # Convert student logits to probabilities with same temperature\n",
        "    s_start_probs = softmax_with_temperature(student_start_logits, TEMPERATURE)\n",
        "    s_end_probs   = softmax_with_temperature(student_end_logits,   TEMPERATURE)\n",
        "\n",
        "    # We'll use KLDivLoss (expects log probs for input by default)\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    start_kl = kl_loss_fn(s_start_probs.log(), t_start_probs)  # KL(student||teacher)\n",
        "    end_kl   = kl_loss_fn(s_end_probs.log(),   t_end_probs)\n",
        "    soft_loss = 0.5 * (start_kl + end_kl)\n",
        "\n",
        "    loss = ALPHA * soft_loss + (1 - ALPHA) * hard_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), hard_loss.item(), soft_loss.item()\n",
        "\n",
        "# Run distillation training\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n=== EPOCH {epoch+1}/{EPOCHS} ===\")\n",
        "    epoch_losses = []\n",
        "    for step, batch_data in enumerate(distill_loader):\n",
        "        loss_val, hard_val, soft_val = distillation_train_step(batch_data)\n",
        "        epoch_losses.append(loss_val)\n",
        "        if step % 200 == 0:\n",
        "            print(f\" Step {step} - Distill Loss: {loss_val:.4f} (Hard: {hard_val:.4f}, Soft: {soft_val:.4f})\")\n",
        "    print(f\" Average Loss: {np.mean(epoch_losses):.4f}\")"
      ],
      "metadata": {
        "id": "i4Gg4Z1xkBhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "###############################################################################\n",
        "# 5) POST-PROCESSING & EVALUATION\n",
        "###############################################################################\n",
        "# We'll do a naive approach: take argmax of start/end, decode, compare with ground truth.\n",
        "val_contexts = val_data[\"context\"]\n",
        "val_questions = val_data[\"question\"]\n",
        "val_answers = val_data[\"answers\"]  # list of dicts with \"text\", \"answer_start\"\n",
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", txt)\n",
        "    def remove_punc(txt):\n",
        "        return \"\".join(ch for ch in txt if ch not in string.punctuation)\n",
        "\n",
        "    s = s.lower()\n",
        "    s = remove_articles(s)\n",
        "    s = remove_punc(s)\n",
        "    s = \" \".join(s.split())\n",
        "    return s\n",
        "\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    if precision + recall == 0:\n",
        "        return 0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "student_model.eval()\n",
        "em_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "val_dataloader = DataLoader(val_tds, batch_size=8, shuffle=False)\n",
        "offset = 0  # to track the global index in val_data\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        if len(batch) == 5:\n",
        "            input_ids, attention_mask, token_type_ids, start_pos, end_pos = batch\n",
        "            token_type_ids = token_type_ids.to(DEVICE)\n",
        "        else:\n",
        "            input_ids, attention_mask, start_pos, end_pos = batch\n",
        "            token_type_ids = None\n",
        "\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attention_mask = attention_mask.to(DEVICE)\n",
        "\n",
        "        outputs = student_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits   = outputs.end_logits\n",
        "\n",
        "        start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "        end_indices   = torch.argmax(end_logits,   dim=1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(start_indices)):\n",
        "            global_idx = offset + i\n",
        "            if global_idx >= len(val_contexts):\n",
        "                continue\n",
        "\n",
        "            s_ind = start_indices[i]\n",
        "            e_ind = end_indices[i]\n",
        "            if e_ind < s_ind:\n",
        "                e_ind = s_ind\n",
        "\n",
        "            # Decode predicted tokens\n",
        "            tokens_ = input_ids[i][s_ind : e_ind+1].cpu().numpy().tolist()\n",
        "            pred_text = tokenizer.decode(tokens_, skip_special_tokens=True)\n",
        "\n",
        "            # Ground truth: we pick the first answer\n",
        "            gold_answers = val_answers[global_idx][\"text\"]\n",
        "            if len(gold_answers) > 0:\n",
        "                gold_answer = gold_answers[0]\n",
        "            else:\n",
        "                gold_answer = \"\"\n",
        "\n",
        "            em = compute_exact_match(pred_text, gold_answer)\n",
        "            f1 = compute_f1(pred_text, gold_answer)\n",
        "\n",
        "            em_scores.append(em)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "        offset += len(start_indices)\n",
        "\n",
        "avg_em = np.mean(em_scores) * 100\n",
        "avg_f1 = np.mean(f1_scores) * 100\n",
        "print(f\"\\nValidation Results (subset of 1000 samples):\")\n",
        "print(f\"  Exact Match: {avg_em:.2f}%\")\n",
        "print(f\"  F1 Score:    {avg_f1:.2f}%\")"
      ],
      "metadata": {
        "id": "aNePOmBgkObU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def ask_question(question: str, context: str, model, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Given a question and a context, use the provided model to\n",
        "    predict the answer span and return the decoded string answer.\n",
        "    \"\"\"\n",
        "    # 1) Encode inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    token_type_ids = None\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # 2) Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # 3) Get predicted start/end token indices\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Ensure the end_index is >= start_index\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    # 4) Decode tokens back to string\n",
        "    answer_ids = input_ids[0, start_index : end_index+1]\n",
        "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    return answer_text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "\n",
        "# Suppose you have:\n",
        "#   teacher_model, student_model (both on the same device, e.g., \"cuda\" or \"cpu\")\n",
        "#   tokenizer (matching your BERT-based QA model)\n",
        "# Example question + context:\n",
        "#question = \"What is the capital of France?\"\n",
        "#context = \"France is a country in Europe. Its largest city and capital is Paris. It is known for the Eiffel Tower.\"\n",
        "#question = \"Which country is Middlesex University based?\"\n",
        "#question = \"Is Middlesex University a public or an independent university?\"\n",
        "#context = \"Middlesex University London is a public research university based in Hendon, northwest London, England.\"\n",
        "question = \"Which city is Galatasaray based in?\"\n",
        "context = \"Galatasaray, is a Turkish professional football club based on the European side of the city of Istanbul. It is founded in 1905. The team traditionally play in dark shades of red and yellow at home.\"\n",
        "\n",
        "# Evaluate with teacher model\n",
        "teacher_model.eval()\n",
        "teacher_answer = ask_question(question, context, teacher_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Teacher Answer]: {teacher_answer}\")\n",
        "\n",
        "# Evaluate with student model\n",
        "student_model.eval()\n",
        "student_answer = ask_question(question, context, student_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Student Answer]: {student_answer}\")"
      ],
      "metadata": {
        "id": "IBd4vVy1kPmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dPTxA2b3kVz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}