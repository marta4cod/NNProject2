{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marta4cod/NNProject2/blob/main/NN2_Final_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uvGz4Pb5hisA"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XyF7c8xtc0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZsP_cSHiH50"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "import numpy as np\n",
        "import string, re\n",
        "\n",
        "import evaluate\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    BertConfig,\n",
        "    BertForQuestionAnswering\n",
        ")\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_G5b1khjpYB"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# CONFIG\n",
        "###############################################################################\n",
        "TEACHER_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "MAX_LENGTH = 384\n",
        "STRIDE = 126\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 6  # Increase for better results\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "# Distillation hyperparams\n",
        "ALPHA = 0.8        # Weight of soft (teacher) loss\n",
        "TEMPERATURE = 5.0  # Temperature for soft logits\n",
        "MAX_ANSWER_LENGTH = 10  # Adjust this number based on typical answer sizes\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# Introducing seeds to minimise variability in F1 and EM scores\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "8ZtJ8aFnTrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# Function to measure time and memory\n",
        "\n",
        "def benchmark_inference(model, dataloader, device, description=\"Model\"):\n",
        "    import time\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            if 'token_type_ids' in batch:\n",
        "                token_type_ids = batch['token_type_ids'].to(device)\n",
        "            else:\n",
        "                token_type_ids = None\n",
        "\n",
        "            _ = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = end_time - start_time\n",
        "    max_memory = torch.cuda.max_memory_allocated() / 1e6  # Convert to MB\n",
        "\n",
        "    print(f\"\\nðŸ”¹ {description} Inference Benchmark:\")\n",
        "    print(f\"   Inference Time: {total_time:.2f} seconds\")\n",
        "    print(f\"   Max GPU Memory Used: {max_memory:.2f} MB\")\n",
        "\n",
        "    return total_time, max_memory\n"
      ],
      "metadata": {
        "id": "nfnOTYHiE_dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# DATA LOADING & PREPROCESSING WITH CHUNKING\n",
        "###############################################################################\n",
        "\n",
        "# load squad Dataset\n",
        "raw_squad = load_dataset(\"squad\")\n",
        "\n",
        "# For demonstration, we'll use the full training set or a subset\n",
        "#train_data = raw_squad[\"train\"]\n",
        "#val_data = raw_squad[\"validation\"].select(range(1000))\n",
        "\n",
        "train_data = raw_squad[\"train\"].select(range(5000))     # 500 samples for training\n",
        "val_data   = raw_squad[\"validation\"].select(range(1000))  # 100 samples for validation\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    contexts = examples[\"context\"]\n",
        "    answers = examples[\"answers\"]\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=MAX_LENGTH,\n",
        "        stride=STRIDE,\n",
        "        padding=\"max_length\",\n",
        "        truncation=\"only_second\",\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    sample_mapping = encoding[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = encoding[\"offset_mapping\"]\n",
        "    input_ids_batch = encoding[\"input_ids\"]\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = input_ids_batch[i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = answers[sample_index]\n",
        "\n",
        "        if len(answer[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            continue\n",
        "\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "        token_start_index = 0\n",
        "        token_end_index = len(offsets) - 1\n",
        "\n",
        "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "            token_start_index += 1\n",
        "        while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        start_positions.append(token_start_index - 1)\n",
        "        end_positions.append(token_end_index + 1)\n",
        "\n",
        "    encoding[\"start_positions\"] = start_positions\n",
        "    encoding[\"end_positions\"] = end_positions\n",
        "    encoding.pop(\"offset_mapping\")\n",
        "\n",
        "    return encoding\n",
        "\n",
        "\n",
        "# Apply preprocessing\n",
        "train_processed = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
        "val_processed   = val_data.map(preprocess_function, batched=True, remove_columns=val_data.column_names)\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "train_processed.set_format(type=\"torch\")\n",
        "val_processed.set_format(type=\"torch\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_processed, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_processed, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "_y3tSgckmA9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aET8hRx1j0Zt"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# 2) TEACHER MODEL INFERENCE (collect teacher logits)\n",
        "###############################################################################\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(TEACHER_MODEL_NAME).to(DEVICE)\n",
        "teacher_model.eval()\n",
        "\n",
        "#evaluating model\n",
        "benchmark_inference(teacher_model, val_loader, DEVICE, description=\"Teacher Model with Chunking\")\n",
        "\n",
        "teacher_start_logits_list = []\n",
        "teacher_end_logits_list   = []\n",
        "gt_start_list = []\n",
        "gt_end_list   = []\n",
        "input_tensors = []\n",
        "count=0\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "      count+=1\n",
        "      print(\"batch\", count)\n",
        "\n",
        "      input_ids = batch['input_ids'].squeeze(1).to(DEVICE)\n",
        "      attention_mask = batch['attention_mask'].squeeze(1).to(DEVICE)\n",
        "\n",
        "      # batch can have 4 or 5 tensors depending on token_type_ids existence\n",
        "      if 'token_type_ids' in batch:\n",
        "        token_type_ids = batch['token_type_ids'].squeeze(1).to(DEVICE)\n",
        "      else:\n",
        "        token_type_ids = None\n",
        "\n",
        "      start_pos = batch['start_positions'].to(DEVICE)\n",
        "      end_pos = batch['end_positions'].to(DEVICE)\n",
        "\n",
        "      outputs = teacher_model(\n",
        "          input_ids=input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          token_type_ids=token_type_ids\n",
        "      )\n",
        "      # Collect logits\n",
        "      teacher_start_logits_list.append(outputs.start_logits.cpu())\n",
        "      teacher_end_logits_list.append(outputs.end_logits.cpu())\n",
        "\n",
        "      gt_start_list.append(start_pos.cpu())\n",
        "      gt_end_list.append(end_pos.cpu())\n",
        "\n",
        "      # We'll store the CPU tensors of inputs for the student dataset\n",
        "      if token_type_ids is not None:\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), token_type_ids.cpu()))\n",
        "      else:\n",
        "          # Rare for BERT-based QA to not have token_type_ids, but just in case:\n",
        "          # We'll store None in place of token_type_ids.\n",
        "          input_tensors.append((input_ids.cpu(), attention_mask.cpu(), None))\n",
        "\n",
        "# Concatenate teacher outputs\n",
        "teacher_start_logits_full = torch.cat(teacher_start_logits_list, dim=0)\n",
        "teacher_end_logits_full   = torch.cat(teacher_end_logits_list, dim=0)\n",
        "gt_start_full = torch.cat(gt_start_list, dim=0)\n",
        "gt_end_full   = torch.cat(gt_end_list, dim=0)\n",
        "\n",
        "# Flatten input tensors\n",
        "all_input_ids         = []\n",
        "all_attention_masks   = []\n",
        "all_token_type_ids    = []\n",
        "for i_ids, i_mask, i_type in input_tensors:\n",
        "    all_input_ids.append(i_ids)\n",
        "    all_attention_masks.append(i_mask)\n",
        "    if i_type is not None:\n",
        "        all_token_type_ids.append(i_type)\n",
        "\n",
        "all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "if all_token_type_ids is not None and len(all_token_type_ids) > 0:\n",
        "    all_token_type_ids = torch.cat(all_token_type_ids, dim=0)\n",
        "else:\n",
        "    all_token_type_ids = None\n",
        "\n",
        "print(\"Teacher logits shapes:\")\n",
        "print(\"  start_logits:\", teacher_start_logits_full.shape)\n",
        "print(\"  end_logits:  \", teacher_end_logits_full.shape)\n",
        "print(\"Ground truth shapes:\", gt_start_full.shape, gt_end_full.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I19QFe_6j7fY"
      },
      "outputs": [],
      "source": [
        "\n",
        "###############################################################################\n",
        "# 3) DEFINE A SMALLER STUDENT MODEL\n",
        "###############################################################################\n",
        "student_config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=384,         # smaller hidden dim\n",
        "    num_hidden_layers=8,     # fewer layers\n",
        "    num_attention_heads=8,   # fewer heads\n",
        "    intermediate_size=384 * 4,\n",
        "    max_position_embeddings=MAX_LENGTH\n",
        ")\n",
        "student_model = BertForQuestionAnswering(student_config).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model size\n",
        "torch.save(student_model.state_dict(), \"student_model_temp.pth\")\n",
        "\n",
        "import os\n",
        "\n",
        "size_mb = os.path.getsize(\"student_model_temp.pth\") / 1e6\n",
        "print(f\"Student model size: {size_mb:.2f} MB\")\n",
        "\n",
        "os.remove(\"student_model_temp.pth\")\n",
        "\n",
        "total_params = sum(p.numel() for p in student_model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "bWbaMn0VVjRr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4Gg4Z1xkBhb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# 4) DISTILLATION TRAINING\n",
        "###############################################################################\n",
        "\n",
        "def softmax_with_temperature(logits, temperature=TEMPERATURE):\n",
        "    return F.softmax(logits / temperature, dim=-1)\n",
        "\n",
        "# Prepare teacher soft targets\n",
        "teacher_start_probs = softmax_with_temperature(teacher_start_logits_full, TEMPERATURE)\n",
        "teacher_end_probs   = softmax_with_temperature(teacher_end_logits_full, TEMPERATURE)\n",
        "\n",
        "# Custom Dataset\n",
        "class DistillationDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids,\n",
        "                 teacher_start_probs, teacher_end_probs,\n",
        "                 gt_start, gt_end):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.teacher_start_probs = teacher_start_probs\n",
        "        self.teacher_end_probs   = teacher_end_probs\n",
        "        self.gt_start = gt_start\n",
        "        self.gt_end   = gt_end\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.input_ids[idx],\n",
        "            self.attention_mask[idx],\n",
        "            self.token_type_ids[idx] if self.token_type_ids is not None else None,\n",
        "            self.teacher_start_probs[idx],\n",
        "            self.teacher_end_probs[idx],\n",
        "            self.gt_start[idx],\n",
        "            self.gt_end[idx],\n",
        "        )\n",
        "\n",
        "# DataLoader and Optimizer\n",
        "distill_dataset = DistillationDataset(\n",
        "    all_input_ids, all_attention_masks, all_token_type_ids,\n",
        "    teacher_start_probs, teacher_end_probs,\n",
        "    gt_start_full, gt_end_full\n",
        ")\n",
        "distill_loader = DataLoader(distill_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training Step\n",
        "def distillation_train_step(batch_data):\n",
        "    input_ids, attention_mask, token_type_ids, t_start_probs, t_end_probs, gt_start, gt_end = batch_data\n",
        "\n",
        "    input_ids = input_ids.to(DEVICE)\n",
        "    attention_mask = attention_mask.to(DEVICE)\n",
        "    t_start_probs = t_start_probs.to(DEVICE)\n",
        "    t_end_probs   = t_end_probs.to(DEVICE)\n",
        "    gt_start = gt_start.to(DEVICE)\n",
        "    gt_end   = gt_end.to(DEVICE)\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = student_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids\n",
        "    )\n",
        "    student_start_logits = outputs.start_logits\n",
        "    student_end_logits   = outputs.end_logits\n",
        "\n",
        "    # Ensure targets are within valid range\n",
        "    seq_len = student_start_logits.shape[1]\n",
        "    gt_start = torch.clamp(gt_start, min=0, max=seq_len - 1)\n",
        "    gt_end   = torch.clamp(gt_end, min=0, max=seq_len - 1)\n",
        "\n",
        "    # Hard loss (ground truth)\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    loss_start_hard = ce_loss_fn(student_start_logits, gt_start)\n",
        "    loss_end_hard   = ce_loss_fn(student_end_logits,   gt_end)\n",
        "\n",
        "    hard_loss = 0.5 * (loss_start_hard + loss_end_hard)\n",
        "\n",
        "    # Soft loss (distillation)\n",
        "    s_start_probs = softmax_with_temperature(student_start_logits, TEMPERATURE)\n",
        "    s_end_probs   = softmax_with_temperature(student_end_logits,   TEMPERATURE)\n",
        "\n",
        "    kl_loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    start_kl = kl_loss_fn((s_start_probs + 1e-8).log(), t_start_probs)\n",
        "    end_kl   = kl_loss_fn((s_end_probs + 1e-8).log(),   t_end_probs)\n",
        "    soft_loss = 0.5 * (start_kl + end_kl)\n",
        "\n",
        "    loss = ALPHA * soft_loss + (1 - ALPHA) * hard_loss\n",
        "\n",
        "    if torch.isnan(loss):\n",
        "        print(\"NaN detected in loss! Skipping batch.\")\n",
        "        return None, None, None\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), hard_loss.item(), soft_loss.item()\n",
        "\n",
        "# Training Loop Example (1 epoch for demo)\n",
        "for step, batch_data in enumerate(distill_loader):\n",
        "    loss_val, hard_val, soft_val = distillation_train_step(batch_data)\n",
        "    if loss_val is not None and step % 100 == 0:\n",
        "        print(f\"Step {step} | Loss: {loss_val:.4f} | Hard: {hard_val:.4f} | Soft: {soft_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize_text(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "    def remove_articles(txt):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', txt)\n",
        "\n",
        "    def remove_punc(txt):\n",
        "        return ''.join(ch for ch in txt if ch not in string.punctuation)\n",
        "\n",
        "    s = s.lower()\n",
        "    s = remove_articles(s)\n",
        "    s = remove_punc(s)\n",
        "    return ' '.join(s.split())\n",
        "\n",
        "def compute_exact_match(pred, truth):\n",
        "    return int(normalize_text(pred) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(pred, truth):\n",
        "    pred_tokens = normalize_text(pred).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    return 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n"
      ],
      "metadata": {
        "id": "-khZP98gqbhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Post-Processing Heuristic: Trim Noisy Outputs\n",
        "## Simple post-processing to clean predicted answers:\n",
        "     ##Trim at first punctuation.\n",
        "     ## Remove subword tokens (##).\n",
        "     ## Strip extra spaces.\n",
        "\n",
        "def clean_predicted_answer(answer):\n",
        "    # Remove subword markers\n",
        "    answer = answer.replace(\"##\", \"\")\n",
        "\n",
        "    # Ensure consistent spacing\n",
        "    answer = answer.strip()\n",
        "\n",
        "    # Trim at first punctuation if present\n",
        "    trimmed = re.split(r'\\s*[.,;!?]\\s*', answer)[0]\n",
        "\n",
        "    return trimmed.strip()"
      ],
      "metadata": {
        "id": "_RbO4SnAO2oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################################\n",
        "######### FUNCTION FOR EVALUATION AND PLOTTING ################\n",
        "\n",
        "# Load the official SQuAD evaluation metric\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "def evaluate_student_model(student_model, val_dataloader, val_data, tokenizer, num_examples_to_save=5):\n",
        "    predictions = []\n",
        "    references = []\n",
        "    offset = 0\n",
        "\n",
        "    val_contexts = val_data[\"context\"]\n",
        "    val_questions = val_data[\"question\"]\n",
        "    val_answers  = val_data[\"answers\"]\n",
        "\n",
        "    # List to store examples for later display\n",
        "    example_predictions = []\n",
        "\n",
        "    student_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            start_pos = batch['start_positions'].to(DEVICE)\n",
        "            end_pos = batch['end_positions'].to(DEVICE)\n",
        "\n",
        "            token_type_ids = batch.get('token_type_ids')\n",
        "            if token_type_ids is not None:\n",
        "                token_type_ids = token_type_ids.to(DEVICE)\n",
        "\n",
        "            outputs = student_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids\n",
        "            )\n",
        "            start_logits = outputs.start_logits\n",
        "            end_logits   = outputs.end_logits\n",
        "\n",
        "            start_indices = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
        "            end_indices   = torch.argmax(end_logits,   dim=1).cpu().numpy()\n",
        "\n",
        "            for i in range(len(start_indices)):\n",
        "                global_idx = offset + i\n",
        "                if global_idx >= len(val_contexts):\n",
        "                    continue\n",
        "\n",
        "                s_ind = start_indices[i]\n",
        "                e_ind = end_indices[i]\n",
        "                if e_ind < s_ind:\n",
        "                    e_ind = s_ind\n",
        "\n",
        "                # Apply Max Span Length\n",
        "                MAX_ANSWER_LENGTH = 10\n",
        "                if (e_ind - s_ind + 1) > MAX_ANSWER_LENGTH:\n",
        "                    e_ind = s_ind + MAX_ANSWER_LENGTH - 1\n",
        "\n",
        "                e_ind = min(e_ind, input_ids.shape[1] - 1)\n",
        "\n",
        "                tokens_ = input_ids[i][s_ind : e_ind+1].cpu().numpy().tolist()\n",
        "\n",
        "                # Decode tokens\n",
        "                pred_text = tokenizer.decode(tokens_, skip_special_tokens=True)\n",
        "\n",
        "                # Apply post-processing\n",
        "                pred_text = clean_predicted_answer(pred_text)\n",
        "\n",
        "\n",
        "                gold_answers = val_answers[global_idx][\"text\"]\n",
        "\n",
        "                if len(gold_answers) == 0:\n",
        "                    reference_texts = [\"\"]\n",
        "                else:\n",
        "                    reference_texts = gold_answers\n",
        "\n",
        "                # Prepare for official SQuAD metric\n",
        "                predictions.append({\n",
        "                    \"id\": str(global_idx),\n",
        "                    \"prediction_text\": pred_text\n",
        "                })\n",
        "\n",
        "                references.append({\n",
        "                    \"id\": str(global_idx),\n",
        "                    \"answers\": {\"text\": reference_texts, \"answer_start\": [0]*len(reference_texts)}  # start not used here\n",
        "                })\n",
        "\n",
        "                # Save sample predictions\n",
        "                if len(example_predictions) < num_examples_to_save:\n",
        "                    example_predictions.append({\n",
        "                        \"question\": val_questions[global_idx],\n",
        "                        \"predicted\": pred_text,\n",
        "                        \"actual\": reference_texts\n",
        "                    })\n",
        "\n",
        "            offset += len(start_indices)\n",
        "\n",
        "    # Compute official SQuAD metrics\n",
        "    results = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    print(f\"\\nOfficial SQuAD Evaluation:\")\n",
        "    print(f\"  Exact Match: {results['exact_match']:.2f}%\")\n",
        "    print(f\"  F1 Score:    {results['f1']:.2f}%\")\n",
        "\n",
        "    return results['exact_match'], results['f1'], example_predictions\n"
      ],
      "metadata": {
        "id": "0uKe_QpRv7FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "########## DISTILATION TRACKING WITH EVALUATION #################\n",
        "\n",
        "em_tracking = []\n",
        "f1_tracking = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n=== EPOCH {epoch+1}/{EPOCHS} ===\")\n",
        "    epoch_losses = []\n",
        "\n",
        "    for step, batch_data in enumerate(distill_loader):\n",
        "        loss_val, hard_val, soft_val = distillation_train_step(batch_data)\n",
        "        epoch_losses.append(loss_val)\n",
        "        if step % 200 == 0:\n",
        "            print(f\" Step {step} - Distill Loss: {loss_val:.4f} (Hard: {hard_val:.4f}, Soft: {soft_val:.4f})\")\n",
        "\n",
        "    print(f\" Average Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "    # Evaluate after epoch\n",
        "    avg_em, avg_f1, samples = evaluate_student_model(student_model, val_loader, val_data, tokenizer)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Validation:\")\n",
        "    print(f\"  Exact Match: {avg_em:.2f}%\")\n",
        "    print(f\"  F1 Score:    {avg_f1:.2f}%\")\n",
        "\n",
        "    em_tracking.append(avg_em)\n",
        "    f1_tracking.append(avg_f1)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0eeSFfK9v_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Plot F1 and EM Over Epochs #####\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(range(1, EPOCHS+1), f1_tracking, label=\"F1 Score\", marker='o')\n",
        "plt.plot(range(1, EPOCHS+1), em_tracking, label=\"Exact Match\", marker='s')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score (%)\")\n",
        "plt.title(\"F1 and EM Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save before showing\n",
        "plt.savefig(\"f1_em_over_epochs.jpg\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qruEGiQkwFWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"f1_em_over_epochs.jpg\")"
      ],
      "metadata": {
        "id": "coXhu1RRR9A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBd4vVy1kPmu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def ask_question(question: str, context: str, model, tokenizer, device=DEVICE): # change device\n",
        "    \"\"\"\n",
        "    Given a question and a context, use the provided model to\n",
        "    predict the answer span and return the decoded string answer.\n",
        "    \"\"\"\n",
        "    # Encode inputs\n",
        "    inputs = tokenizer(question, context, return_tensors=\"pt\", max_length= 384, truncation=\"only_second\") # changed here\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "    token_type_ids = inputs.get(\"token_type_ids\") # here\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "    # extract logits\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # Get predicted start/end token indices\n",
        "    start_index = torch.argmax(start_logits, dim=1).item()\n",
        "    end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Ensure the end_index is >= start_index\n",
        "    if end_index < start_index:\n",
        "        end_index = start_index\n",
        "\n",
        "    # Apply max Span Length\n",
        "    max_span_length = 10\n",
        "    if (end_index - start_index + 1) > max_span_length:\n",
        "        end_index = start_index + max_span_length - 1\n",
        "\n",
        "    # prevent going beyong sequence length\n",
        "    end_index = min(end_index, len(input_ids[0]) - 1)\n",
        "\n",
        "    # 4) Decode tokens back to string\n",
        "    answer_ids = input_ids[0, start_index : end_index+1]\n",
        "    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate simple confidence score (average of start & end probabilities)\n",
        "    start_prob = torch.softmax(start_logits, dim=1)[0, start_index].item()\n",
        "    end_prob   = torch.softmax(end_logits, dim=1)[0, end_index].item()\n",
        "    confidence = (start_prob + end_prob) / 2\n",
        "\n",
        "    return answer_text, confidence\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Example usage\n",
        "# -----------------------------\n",
        "\n",
        "# Suppose you have:\n",
        "#   teacher_model, student_model (both on the same device, e.g., \"cuda\" or \"cpu\")\n",
        "#   tokenizer (matching your BERT-based QA model)\n",
        "# Example question + context:\n",
        "#question = \"What is the capital of France?\"\n",
        "#context = \"France is a country in Europe. Its largest city and capital is Paris. It is known for the Eiffel Tower.\"\n",
        "#question = \"Which country is Middlesex University based?\"\n",
        "#question = \"Is Middlesex University a public or an independent university?\"\n",
        "#context = \"Middlesex University London is a public research university based in Hendon, northwest London, England.\"\n",
        "question = \"Which city is Galatasaray based in?\"\n",
        "context = \"Galatasaray, is a Turkish professional football club based on the European side of the city of Istanbul. It is founded in 1905. The team traditionally play in dark shades of red and yellow at home.\"\n",
        "\n",
        "# Evaluate with teacher model\n",
        "teacher_model.eval()\n",
        "teacher_answer = ask_question(question, context, teacher_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Teacher Answer]: {teacher_answer}\")\n",
        "\n",
        "# Evaluate with student model\n",
        "student_model.eval()\n",
        "student_answer = ask_question(question, context, student_model, tokenizer, device=\"cuda\")\n",
        "print(f\"[Student Answer]: {student_answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPTxA2b3kVz1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyOpYP0JBd/IOlBEXJEoCAd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
